{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate modle code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def division_function(n, d):\n",
    "    if d:\n",
    "        return n / d\n",
    "    elif n == 0 and d == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_model(model, X, Y, fold):\n",
    "    \"\"\"\n",
    "    validates the model with a k-fold validation which is iterated\n",
    "    returns the mean accuracy, specificiy, recall, precision, f1 score and auc score\n",
    "    \"\"\"\n",
    "\n",
    "    splits = 5\n",
    "    iteration = 10\n",
    "\n",
    "    acc_list = []\n",
    "    specificity_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    if fold == \"Strat\":\n",
    "        folds = StratifiedKFold(n_splits=splits)\n",
    "    elif fold == \"K\":\n",
    "        folds = KFold(splits, shuffle=True)\n",
    "\n",
    "    # Iterate \"interation\" times of k-fold\n",
    "    for i in range(1, iteration):\n",
    "        # print(f'Iteration {i}/{iteration}')\n",
    "\n",
    "        acc_total = 0\n",
    "        specificity_total = 0\n",
    "        recall_total = 0\n",
    "        precision_total = 0\n",
    "        f1_total = 0\n",
    "\n",
    "        for train_index, test_index in folds.split(X, Y):\n",
    "            x_train = X.iloc[train_index, :]\n",
    "            x_test = X.iloc[test_index, :]\n",
    "            y_train = Y.iloc[train_index, :]\n",
    "            y_test = Y.iloc[test_index, :]\n",
    "\n",
    "            # scale\n",
    "            sc = MinMaxScaler()\n",
    "            x_train = sc.fit_transform(x_train)\n",
    "            x_test = sc.transform(x_test)\n",
    "\n",
    "            # fit model and predict\n",
    "            model.fit(x_train, np.ravel(y_train))\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            TN = conf_matrix[0][0]\n",
    "            FP = conf_matrix[0][1]\n",
    "            FN = conf_matrix[1][0]\n",
    "            TP = conf_matrix[1][1]\n",
    "\n",
    "            accuracy = (division_function((TP + TN), (TP + TN + FP + FN))) * 100\n",
    "            specificity = division_function(TN, (TN + FP)) * 100\n",
    "            recall = division_function(TP, (TP + FN)) * 100  # recall\n",
    "            precision = division_function(TP, (TP + FP)) * 100\n",
    "            f1_score = division_function(2 * (recall * precision), (recall + precision))\n",
    "\n",
    "            # sum it up\n",
    "            acc_total += accuracy\n",
    "            specificity_total += specificity\n",
    "            recall_total += recall\n",
    "            precision_total += precision\n",
    "            f1_total += f1_score\n",
    "\n",
    "        # avg\n",
    "        accuracy_mean = acc_total / splits\n",
    "        recall_mean = recall_total / splits\n",
    "        specificity_mean = specificity_total / splits\n",
    "        precision_mean = precision_total / splits\n",
    "        f1_mean = f1_total / splits\n",
    "\n",
    "        acc_list.append(accuracy_mean)\n",
    "        recall_list.append(recall_mean)\n",
    "        specificity_list.append(specificity_mean)\n",
    "        precision_list.append(precision_mean)\n",
    "        f1_list.append(f1_mean)\n",
    "\n",
    "    return (\n",
    "        np.mean(acc_list),\n",
    "        np.mean(specificity_list),\n",
    "        np.mean(recall_list),\n",
    "        np.mean(precision_list),\n",
    "        np.mean(f1_list)\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate modle code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_train, x_test, y_train, y_test):\n",
    "    sc = MinMaxScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    TP = conf_matrix[1][1]\n",
    "\n",
    "    accuracy = (division_function((TP + TN), (TP + TN + FP + FN))) * 100\n",
    "    specificity = division_function(TN, (TN + FP)) * 100\n",
    "    precision = division_function(TP, (TP + FP)) * 100\n",
    "    recall = division_function(TP, (TP + FN)) * 100  # recall\n",
    "    f1_score = division_function(2 * (recall * precision), (recall + precision))\n",
    "\n",
    "    return accuracy, specificity, recall, precision, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_reduction import feature_reduction_lda, feature_reduction_mrmr\n",
    "from imblearn.over_sampling import SMOTE\n",
    "path = '/Users/athena.kam/Documents/Thesis/codebase/thesis-2023-athena'\n",
    "os.chdir(path)\n",
    "\n",
    "CV_SPLIT = 5\n",
    "\n",
    "def get_best_param_RF(x_train, y_train):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth' : [4,5,6,7,8],\n",
    "        'criterion' :['gini', 'entropy']\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        RandomForestClassifier(),\n",
    "        param_grid,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        return_train_score=True,\n",
    "        cv=CV_SPLIT\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(grid.best_estimator_.get_params())\n",
    "    return grid\n",
    "\n",
    "\n",
    "def train_test_RF(filename:str,hold_out:bool = True,include_personal_q:bool = False , grid_search:bool = True,reduce:bool = True,over_sample:bool = False,model_weights:dict = {},random_state:int = 0):\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    if include_personal_q:\n",
    "        df = df[df['noPersonalQ']!=1].reset_index(drop=True)\n",
    "    else:\n",
    "        df = df[df['personalQ']!=1].reset_index(drop=True)\n",
    "    \n",
    "    headers = df.columns\n",
    "    non_embeddings_headers = []\n",
    "    \n",
    "    for header in headers:\n",
    "        if header.find('embbedings')<0:\n",
    "            non_embeddings_headers.append(header)\n",
    "\n",
    "    X = df.drop(columns=non_embeddings_headers)\n",
    "    Y = df['classification']\n",
    "\n",
    "    # Feature Reduction \n",
    "    if reduce:\n",
    "        #x_train = feature_reduction_pca(x_train,0.9).values\n",
    "        #X = feature_reduction_lda(X,Y)\n",
    "        X = feature_reduction_mrmr(X,Y,20)\n",
    "        \n",
    "    else:\n",
    "        x_val = X.values\n",
    "        X = StandardScaler().fit_transform(x_val)\n",
    "    \n",
    "    # Test Train split\n",
    "    if hold_out:\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.20,random_state=random_state)\n",
    "\n",
    "        # Oversample on training set\n",
    "        if over_sample:\n",
    "            sm = SMOTE(random_state=12)\n",
    "            x_train,y_train = sm.fit_resample(x_train, y_train) \n",
    "    else:\n",
    "        x_train = X\n",
    "        y_train = Y\n",
    "\n",
    "\n",
    "    if grid_search: \n",
    "        grid = get_best_param_RF(x_train=x_train,y_train=y_train) \n",
    "        model_svc = grid.best_estimator_\n",
    "    else:\n",
    "        model_svc = RandomForestClassifier(C = model_weights['C'],gamma=model_weights['gamma'],kernel=model_weights['kernel'])\n",
    "        \n",
    "    accuracy, specificiy, recall, precision, f1_score =validate_model(model_svc,pd.DataFrame(x_train),pd.DataFrame(y_train),\"Strat\")\n",
    "    print(f'\\tAverage Accuracy: {accuracy} \\n\\\n",
    "      Average Specificity: {specificiy} \\n\\\n",
    "      Average Recall: {recall}\\n\\\n",
    "      Average Precision:{precision}\\n\\\n",
    "      Average F1 score {f1_score}\\n\\\n",
    "      ')\n",
    "    \n",
    "    if hold_out:\n",
    "        accuracy, specificiy, recall, precision, f1_score =evaluate_model(model_svc,x_train,x_test,y_train,y_test)\n",
    "        print('____________________________________________')\n",
    "        print('Evaluate model')\n",
    "        print(f'\\tAccuracy: {accuracy} \\n\\\n",
    "        Specificity: {specificiy} \\n\\\n",
    "        Recall: {recall}\\n\\\n",
    "        Precision:{precision}\\n\\\n",
    "        F1 score {f1_score}\\n\\\n",
    "        ')\n",
    "        return accuracy, specificiy, recall, precision, f1_score\n",
    "    else: \n",
    "        return accuracy, specificiy, recall, precision, f1_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing different seeds (Hold out only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_TEST_avg_seeds(\n",
    "    filename: str,\n",
    "    hold_out: bool = True,\n",
    "    include_personal_q: bool = False,\n",
    "    grid_search: bool = True,\n",
    "    reduce: bool = True,\n",
    "    model_weights: dict = {},\n",
    "    over_sample: bool = False,\n",
    "):\n",
    "    random_states = [0, 5, 13, 27, 36, 42]\n",
    "    n_states = len(random_states)\n",
    "\n",
    "    acc_list = []\n",
    "    specificity_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for random_state in random_states:\n",
    "        print(f\"Random State: {random_state}\")\n",
    "        accuracy, specificity, recall, precision, f1_score = train_test_RF(\n",
    "            filename=filename,\n",
    "            hold_out=hold_out,\n",
    "            include_personal_q=include_personal_q,\n",
    "            grid_search=grid_search,\n",
    "            reduce=reduce,\n",
    "            model_weights=model_weights,\n",
    "            random_state=random_state,\n",
    "            over_sample=over_sample,\n",
    "        )\n",
    "        acc_list.append(accuracy)\n",
    "        specificity_list.append(specificity)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        f1_list.append(f1_score)\n",
    "\n",
    "    accuracy_mean = np.mean(acc_list)\n",
    "    specificity_mean = np.mean(specificity_list)\n",
    "    recall_mean = np.mean(recall_list)\n",
    "    precision_mean = np.mean(precision_list)\n",
    "    f1_mean = np.mean(f1_list)\n",
    "\n",
    "    print(\"Accuracy list: \", acc_list)\n",
    "    print(\"Specificity list: \", specificity_list)\n",
    "    print(\"Recall list: \", recall_list)\n",
    "    print(\"Precision list: \", precision_list)\n",
    "    print(\"F1  score list: \", f1_list)\n",
    "\n",
    "    print(\n",
    "        f\"\\tAverage Accuracy: {accuracy_mean} \\n\\\n",
    "      Average Specificity: {specificity_mean} \\n\\\n",
    "      Average Recall: {recall_mean}\\n\\\n",
    "      Average Precision:{precision_mean}\\n\\\n",
    "      Average F1 score {f1_mean}\\n\\\n",
    "      \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 8, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\tAverage Accuracy: 92.22222222222223 \n",
      "      Average Specificity: 88.33333333333333 \n",
      "      Average Recall: 97.22222222222223\n",
      "      Average Precision:90.0\n",
      "      Average F1 score 92.69841269841271\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 87.5 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 66.66666666666666\n",
      "        Precision:100.0\n",
      "        F1 score 80.0\n",
      "        \n",
      "Random State: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 4, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\tAverage Accuracy: 82.48677248677248 \n",
      "      Average Specificity: 78.88888888888887 \n",
      "      Average Recall: 88.7037037037037\n",
      "      Average Precision:85.55555555555556\n",
      "      Average F1 score 83.32804232804234\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 100.0 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 100.0\n",
      "        Precision:100.0\n",
      "        F1 score 100.0\n",
      "        \n",
      "Random State: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\tAverage Accuracy: 94.44444444444446 \n",
      "      Average Specificity: 93.33333333333333 \n",
      "      Average Recall: 95.0\n",
      "      Average Precision:94.66666666666667\n",
      "      Average F1 score 94.17989417989416\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 37.5 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 28.57142857142857\n",
      "        Precision:100.0\n",
      "        F1 score 44.44444444444444\n",
      "        \n",
      "Random State: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 7, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\tAverage Accuracy: 89.20634920634922 \n",
      "      Average Specificity: 83.88888888888889 \n",
      "      Average Recall: 94.44444444444443\n",
      "      Average Precision:87.48148148148147\n",
      "      Average F1 score 89.83245149911816\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 100.0 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 100.0\n",
      "        Precision:100.0\n",
      "        F1 score 100.0\n",
      "        \n",
      "Random State: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\tAverage Accuracy: 87.46031746031747 \n",
      "      Average Specificity: 81.66666666666666 \n",
      "      Average Recall: 93.88888888888889\n",
      "      Average Precision:86.44444444444444\n",
      "      Average F1 score 88.51851851851853\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 87.5 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 66.66666666666666\n",
      "        Precision:100.0\n",
      "        F1 score 80.0\n",
      "        \n",
      "Random State: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\tAverage Accuracy: 90.0 \n",
      "      Average Specificity: 91.85185185185185 \n",
      "      Average Recall: 88.14814814814814\n",
      "      Average Precision:92.77777777777777\n",
      "      Average F1 score 89.65079365079366\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 87.5 \n",
      "        Specificity: 85.71428571428571 \n",
      "        Recall: 100.0\n",
      "        Precision:50.0\n",
      "        F1 score 66.66666666666667\n",
      "        \n",
      "Accuracy list:  [87.5, 100.0, 37.5, 100.0, 87.5, 87.5]\n",
      "Specificity list:  [100.0, 100.0, 100.0, 100.0, 100.0, 85.71428571428571]\n",
      "Recall list:  [66.66666666666666, 100.0, 28.57142857142857, 100.0, 66.66666666666666, 100.0]\n",
      "Precision list:  [100.0, 100.0, 100.0, 100.0, 100.0, 50.0]\n",
      "F1  score list:  [80.0, 100.0, 44.44444444444444, 100.0, 80.0, 66.66666666666667]\n",
      "\tAverage Accuracy: 83.33333333333333 \n",
      "      Average Specificity: 97.6190476190476 \n",
      "      Average Recall: 76.98412698412699\n",
      "      Average Precision:91.66666666666667\n",
      "      Average F1 score 78.51851851851852\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "train_test_TEST_avg_seeds('datasets/transformed/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv',over_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "\tAverage Accuracy: 88.0 \n",
      "      Average Specificity: 86.66666666666666 \n",
      "      Average Recall: 90.0\n",
      "      Average Precision:86.66666666666666\n",
      "      Average F1 score 85.33333333333334\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 90.9090909090909 \n",
      "        Specificity: 83.33333333333334 \n",
      "        Recall: 100.0\n",
      "        Precision:83.33333333333334\n",
      "        F1 score 90.9090909090909\n",
      "        \n",
      "Random State: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'manhattan', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "\tAverage Accuracy: 92.0 \n",
      "      Average Specificity: 100.0 \n",
      "      Average Recall: 83.33333333333333\n",
      "      Average Precision:100.0\n",
      "      Average F1 score 89.33333333333334\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 100.0 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 100.0\n",
      "        Precision:100.0\n",
      "        F1 score 100.0\n",
      "        \n",
      "Random State: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}\n",
      "\tAverage Accuracy: 84.0 \n",
      "      Average Specificity: 100.0 \n",
      "      Average Recall: 30.0\n",
      "      Average Precision:40.0\n",
      "      Average F1 score 33.333333333333336\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 45.45454545454545 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 25.0\n",
      "        Precision:100.0\n",
      "        F1 score 40.0\n",
      "        \n",
      "Random State: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}\n",
      "\tAverage Accuracy: 76.0 \n",
      "      Average Specificity: 80.0 \n",
      "      Average Recall: 76.66666666666666\n",
      "      Average Precision:80.0\n",
      "      Average F1 score 71.33333333333334\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 100.0 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 100.0\n",
      "        Precision:100.0\n",
      "        F1 score 100.0\n",
      "        \n",
      "Random State: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'manhattan', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}\n",
      "\tAverage Accuracy: 96.0 \n",
      "      Average Specificity: 100.0 \n",
      "      Average Recall: 90.0\n",
      "      Average Precision:100.0\n",
      "      Average F1 score 93.33333333333334\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 90.9090909090909 \n",
      "        Specificity: 100.0 \n",
      "        Recall: 80.0\n",
      "        Precision:100.0\n",
      "        F1 score 88.88888888888889\n",
      "        \n",
      "Random State: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}\n",
      "\tAverage Accuracy: 84.0 \n",
      "      Average Specificity: 86.66666666666666 \n",
      "      Average Recall: 80.0\n",
      "      Average Precision:83.33333333333333\n",
      "      Average F1 score 79.33333333333334\n",
      "      \n",
      "____________________________________________\n",
      "Evaluate model\n",
      "\tAccuracy: 90.9090909090909 \n",
      "        Specificity: 87.5 \n",
      "        Recall: 100.0\n",
      "        Precision:75.0\n",
      "        F1 score 85.71428571428571\n",
      "        \n",
      "Accuracy list:  [90.9090909090909, 100.0, 45.45454545454545, 100.0, 90.9090909090909, 90.9090909090909]\n",
      "Specificity list:  [83.33333333333334, 100.0, 100.0, 100.0, 100.0, 87.5]\n",
      "Recall list:  [100.0, 100.0, 25.0, 100.0, 80.0, 100.0]\n",
      "Precision list:  [83.33333333333334, 100.0, 100.0, 100.0, 100.0, 75.0]\n",
      "F1  score list:  [90.9090909090909, 100.0, 40.0, 100.0, 88.88888888888889, 85.71428571428571]\n",
      "\tAverage Accuracy: 86.36363636363636 \n",
      "      Average Specificity: 95.1388888888889 \n",
      "      Average Recall: 84.16666666666667\n",
      "      Average Precision:93.05555555555556\n",
      "      Average F1 score 84.25204425204426\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "train_test_TEST_avg_seeds('datasets/transformed/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv',over_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
