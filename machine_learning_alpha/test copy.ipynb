{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import (\n",
    "    StandardScaler,\n",
    "    LinearDiscriminantAnalysis as LDA,\n",
    ")\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def feature_reduction_pca(x_train,x_test, variance: float):\n",
    "\n",
    "    pca = PCA(n_components=variance)\n",
    "\n",
    "    x_train = pca.fit_transform(x_train)\n",
    "    x_test = pca.transform(x_test)\n",
    "\n",
    "    return x_train,x_test\n",
    "\n",
    "\n",
    "def feature_reduction_lda(x_train,x_test, y_train):\n",
    "    \"\"\"\n",
    "    LDA is supervised so we need a test and train split\n",
    "    \"\"\"\n",
    "\n",
    "    # LDA\n",
    "    lda = LDA(n_components=1)\n",
    "    x_train = lda.fit_transform(x_train, y_train)\n",
    "    x_test = lda.transform(x_test)\n",
    "\n",
    "    return x_train,x_test\n",
    "\n",
    "\n",
    "def feature_reduction_mrmr(x_train,x_test, y_train, n_components):\n",
    "    selected_components = mrmr_classif(X=x_train, y=y_train, K=n_components)\n",
    "    x_train = pd.DataFrame(x_train).loc[:, selected_components]\n",
    "    x_test = pd.DataFrame(x_test).loc[:, selected_components]\n",
    "    return x_train,x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "CV_SPLIT = 5\n",
    "\n",
    "\"\"\"\n",
    "GridSearch for parameter optimisation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_best_param_RF(x_train, y_train):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [100, 200, 500],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        \"max_depth\": [4, 5, 6, 7, 8],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        RandomForestClassifier(),\n",
    "        param_grid,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        return_train_score=True,\n",
    "        cv=CV_SPLIT,\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(grid.best_estimator_.get_params())\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_best_param_KNN(x_train, y_train):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\": range(1, 21, 2),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        KNeighborsClassifier(),\n",
    "        param_grid,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        return_train_score=True,\n",
    "        cv=CV_SPLIT,\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(grid.best_estimator_.get_params())\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_best_param_LR(x_train, y_train):\n",
    "    param_grid = {\n",
    "        \"C\": [100, 10, 1.0, 0.1, 0.01],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        LogisticRegression(),\n",
    "        param_grid,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        return_train_score=True,\n",
    "        cv=CV_SPLIT,\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(grid.best_estimator_.get_params())\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_best_param_SVC(x_train, y_train):\n",
    "    param_grid = {\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"gamma\": [1, 0.1, 0.01, 0.001],\n",
    "        \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        SVC(), param_grid, refit=True, verbose=0, return_train_score=True, cv=CV_SPLIT\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(grid.best_estimator_.get_params())\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from evaluation_tools import validate_model, evaluate_model\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "\n",
    "path = '/Users/athena.kam/Documents/Thesis/codebase/thesis-2023-athena'\n",
    "os.chdir(path)\n",
    "\n",
    "\"\"\"\n",
    "Reducing the features, splitting the data into test and train, oversample the training data, train the model and validate and evaluate it\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read_and_split(\n",
    "    filename: str, isTranscript: bool, reduce: str, random_state: int, chunked: bool\n",
    "):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    if isTranscript:\n",
    "        # Remove Personal Questions\n",
    "        df = df[df[\"personalQ\"] != 1].reset_index(drop=True)\n",
    "\n",
    "        headers = df.columns\n",
    "        non_embeddings_headers = []\n",
    "        for header in headers:\n",
    "            if header.find(\"embbedings\") < 0:\n",
    "                non_embeddings_headers.append(header)\n",
    "\n",
    "        X = df.drop(columns=non_embeddings_headers)\n",
    "        Y = df[\"classification\"]\n",
    "        X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "        # Train-test split\n",
    "        x_index = range(len(X))\n",
    "        x_train_index, x_test_index, y_train, y_test = train_test_split(\n",
    "            x_index, Y, test_size=0.30, random_state=random_state\n",
    "        )\n",
    "        display(x_train_index)\n",
    "        x_train = pd.DataFrame(X).iloc[x_train_index]\n",
    "        x_test = pd.DataFrame(X).iloc[x_test_index]\n",
    "\n",
    "\n",
    "        # Oversample minority group\n",
    "        sm = SMOTE(random_state=12)\n",
    "        x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "        # Feature Reduction\n",
    "        if reduce == \"pca\":\n",
    "            x_train,x_test = feature_reduction_pca(x_train, x_test, 0.9)\n",
    "        elif reduce == \"lda\":\n",
    "            x_train,x_test = feature_reduction_lda(x_train, x_test, y_train)\n",
    "        elif reduce == \"mrmr\":\n",
    "            x_train,x_test = feature_reduction_mrmr(pd.DataFrame(x_train), pd.DataFrame(x_test), pd.DataFrame(y_train), 30)\n",
    "\n",
    "    else:\n",
    "        if chunked:\n",
    "            df.drop([\"voiceID\", \"label_x\"], inplace=True, axis=1)\n",
    "            df.rename(columns={\"label_y\": \"label\"}, inplace=True)\n",
    "        else:\n",
    "            df.drop([\"voiceID\"], inplace=True, axis=1)\n",
    "        df[\"label\"].value_counts()\n",
    "        df = df.dropna()\n",
    "\n",
    "        df_X = df.iloc[:, :-1]\n",
    "        df_Y = df.iloc[:, -1]\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            df_X, df_Y, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "\n",
    "        sc = MinMaxScaler()\n",
    "        x_train = sc.fit_transform(x_train)\n",
    "        x_test = sc.transform(x_test)\n",
    "        pd.DataFrame(x_train)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def train_model(model_name: str, grid_search: bool, model_weights, x_train, y_train):\n",
    "    if model_name == \"svc\":\n",
    "        if grid_search:\n",
    "            grid = get_best_param_SVC(x_train=x_train, y_train=y_train)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            model = SVC(\n",
    "                C=model_weights[\"C\"],\n",
    "                gamma=model_weights[\"gamma\"],\n",
    "                kernel=model_weights[\"kernel\"],\n",
    "            )\n",
    "    elif model_name == \"lr\":\n",
    "        if grid_search:\n",
    "            grid = get_best_param_LR(x_train=x_train, y_train=y_train)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            model = LogisticRegression(\n",
    "                C=model_weights[\"C\"],\n",
    "                solver=model_weights[\"solver\"],\n",
    "                penalty=model_weights[\"penalty\"],\n",
    "            )\n",
    "    elif model_name == \"knn\":\n",
    "        if grid_search:\n",
    "            grid = get_best_param_KNN(x_train=x_train, y_train=y_train)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            model = KNeighborsClassifier(\n",
    "                n_neighbors=model_weights[\"n_neighbors\"],\n",
    "                weights=model_weights[\"weights\"],\n",
    "                metric=model_weights[\"metric\"],\n",
    "            )\n",
    "    elif model_name == \"rf\":\n",
    "        if grid_search:\n",
    "            grid = get_best_param_RF(x_train=x_train, y_train=y_train)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=model_weights[\"n_estimators\"],\n",
    "                max_features=model_weights[\"max_features\"],\n",
    "                max_depth=model_weights[\"max_depth\"],\n",
    "                criterion=model_weights[\"criterion\"],\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_test_model(\n",
    "    filename: str,\n",
    "    model_name: str,\n",
    "    chunked: bool = False,\n",
    "    reduce: str = \"mrmr\",\n",
    "    isTranscript: bool = True,\n",
    "    grid_search: bool = True,\n",
    "    model_weights: dict = {},\n",
    "    random_state: int = 0,\n",
    "):\n",
    "    x_train, x_test, y_train, y_test = read_and_split(\n",
    "        filename=filename,\n",
    "        isTranscript=isTranscript,\n",
    "        reduce=reduce,\n",
    "        random_state=random_state,\n",
    "        chunked=chunked,\n",
    "    )\n",
    "\n",
    "    # Train ML model\n",
    "    model = train_model(\n",
    "        model_name=model_name,\n",
    "        grid_search=grid_search,\n",
    "        model_weights=model_weights,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "    )\n",
    "\n",
    "    # Validate with training data\n",
    "    accuracy, specificiy, recall, precision, f1_score = validate_model(\n",
    "        model, pd.DataFrame(x_train), pd.DataFrame(y_train)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"\\tAverage Accuracy: {accuracy} \\n\\\n",
    "      Average Specificity: {specificiy} \\n\\\n",
    "      Average Recall: {recall}\\n\\\n",
    "      Average Precision:{precision}\\n\\\n",
    "      Average F1 score {f1_score}\\n\\\n",
    "      \"\n",
    "    )\n",
    "\n",
    "    # Test with test data\n",
    "    accuracy, specificiy, recall, precision, f1_score = evaluate_model(\n",
    "        model=model, x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test\n",
    "    )\n",
    "    print(\"___________________\")\n",
    "    print(\"Evaluate model\")\n",
    "    print(\n",
    "        f\"\\tAccuracy: {accuracy} \\n\\\n",
    "    Specificity: {specificiy} \\n\\\n",
    "    Recall: {recall}\\n\\\n",
    "    Precision:{precision}\\n\\\n",
    "    F1 score {f1_score}\\n\\\n",
    "    \"\n",
    "    )\n",
    "\n",
    "    return accuracy, specificiy, recall, precision, f1_score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\tAverage Accuracy: 53.33333333333333 \n",
      "      Average Specificity: 38.33333333333333 \n",
      "      Average Recall: 71.66666666666666\n",
      "      Average Precision:50.666666666666664\n",
      "      Average F1 score 56.666666666666664\n",
      "      \n",
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 72.72727272727273 \n",
      "    Specificity: 100.0 \n",
      "    Recall: 50.0\n",
      "    Precision:62.5\n",
      "    F1 score 76.92307692307692\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72.72727272727273, 100.0, 50.0, 62.5, 76.92307692307692)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv', model_name= 'lr',reduce='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
