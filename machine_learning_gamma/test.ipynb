{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import (\n",
    "    StandardScaler,\n",
    "    LinearDiscriminantAnalysis as LDA,\n",
    ")\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# def feature_reduction_pca(x_train,x_test, variance: float):\n",
    "\n",
    "#     pca = PCA(n_components=variance)\n",
    "\n",
    "#     x_train = pca.fit_transform(x_train)\n",
    "#     x_test = pca.transform(x_test)\n",
    "\n",
    "#     return x_train,x_test\n",
    "\n",
    "\n",
    "# def feature_reduction_lda(x_train,x_test, y_train):\n",
    "#     \"\"\"\n",
    "#     LDA is supervised so we need a test and train split\n",
    "#     \"\"\"\n",
    "\n",
    "#     # LDA\n",
    "#     lda = LDA(n_components=1)\n",
    "#     x_train = lda.fit_transform(x_train, y_train)\n",
    "#     x_test = lda.transform(x_test)\n",
    "\n",
    "#     return x_train,x_test\n",
    "\n",
    "\n",
    "def feature_reduction_mrmr(x_train,x_test, y_train, n_components):\n",
    "    selected_components = mrmr_classif(X=x_train, y=y_train, K=n_components)\n",
    "    x_train = pd.DataFrame(x_train).loc[:, selected_components]\n",
    "    x_test = pd.DataFrame(x_test).loc[:, selected_components]\n",
    "    return x_train,x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import (\n",
    "    StandardScaler,\n",
    "    LinearDiscriminantAnalysis as LDA,\n",
    ")\n",
    "\n",
    "\n",
    "CV_SPLIT = 5\n",
    "\n",
    "\"\"\"\n",
    "GridSearch for parameter optimisation\n",
    "\"\"\"\n",
    "\n",
    "def get_reducer_variables(reducer_name:str):\n",
    "    if reducer_name == 'pca':\n",
    "        return {'reducer':PCA(),'variables':[0.8,0.9,0.95]}\n",
    "    elif reducer_name == 'lda':\n",
    "        return {'reducer':LDA(),'variables':[1]}\n",
    "\n",
    "    \n",
    "def get_best_param_RF(x_train, y_train,reducer_name):\n",
    "    if reducer_name != 'mrmr':\n",
    "        reducer = get_reducer_variables(reducer_name=reducer_name)\n",
    "        reducer_var = reducer['variables']\n",
    "        reducer_name = reducer['reducer']\n",
    "        \n",
    "        pipe = Pipeline(steps=[('reducer',reducer_name),('rf',RandomForestClassifier())])\n",
    "\n",
    "        param_grid = {\n",
    "            \"rf__n_estimators\": [100, 200, 500],\n",
    "            \"rf__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "            \"rf__max_depth\": [4, 5, 6, 7, 8],\n",
    "            \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "            'reducer__n_components':reducer_var\n",
    "        }\n",
    "        grid = GridSearchCV(pipe,param_grid=param_grid,refit=True, verbose=0, return_train_score=True, cv=CV_SPLIT)\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(grid.best_estimator_.get_params())\n",
    "\n",
    "        return grid\n",
    "    else:\n",
    "        param_grid = {\n",
    "            \"n_estimators\": [100, 200, 500],\n",
    "            \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "            \"max_depth\": [4, 5, 6, 7, 8],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "        }\n",
    "        grid = GridSearchCV(\n",
    "            RandomForestClassifier(),\n",
    "            param_grid,\n",
    "            refit=True,\n",
    "            verbose=0,\n",
    "            return_train_score=True,\n",
    "            cv=CV_SPLIT,\n",
    "        )\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(grid.best_estimator_.get_params())\n",
    "        return grid\n",
    "\n",
    "\n",
    "def get_best_param_KNN(x_train, y_train,reducer_name):\n",
    "    if reducer_name != 'mrmr':\n",
    "        reducer = get_reducer_variables(reducer_name=reducer_name)\n",
    "        reducer_var = reducer['variables']\n",
    "        reducer_name = reducer['reducer']\n",
    "        \n",
    "        pipe = Pipeline(steps=[('reducer',reducer_name),('knn',KNeighborsClassifier())])\n",
    "\n",
    "        param_grid = {\n",
    "           \"knn__n_neighbors\": range(1, 21, 2),\n",
    "           \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "           \"knn__metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "           'reducer__n_components':reducer_var\n",
    "        }\n",
    "        grid = GridSearchCV(pipe,param_grid=param_grid,refit=True, verbose=0, return_train_score=True, cv=CV_SPLIT)\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(grid.best_estimator_.get_params())\n",
    "\n",
    "        return grid\n",
    "    else:\n",
    "        param_grid = {\n",
    "            \"n_neighbors\": range(1, 21, 2),\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "            \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "        }\n",
    "        grid = GridSearchCV(\n",
    "            KNeighborsClassifier(),\n",
    "            param_grid,\n",
    "            refit=True,\n",
    "            verbose=0,\n",
    "            return_train_score=True,\n",
    "            cv=CV_SPLIT,\n",
    "        )\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(grid.best_estimator_.get_params())\n",
    "        return grid\n",
    "\n",
    "\n",
    "def get_best_param_LR(x_train, y_train,reducer_name):\n",
    "\n",
    "    if reducer_name != 'mrmr':\n",
    "        reducer = get_reducer_variables(reducer_name=reducer_name)\n",
    "        reducer_var = reducer['variables']\n",
    "        reducer_name = reducer['reducer']\n",
    "        \n",
    "        pipe = Pipeline(steps=[('reducer',reducer_name),('lr',LogisticRegression())])\n",
    "\n",
    "        param_grid = {\n",
    "            \"lr__C\": [100, 10, 1.0, 0.1, 0.01],\n",
    "            \"lr__solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n",
    "            \"lr__penalty\": [\"l1\", \"l2\"],\n",
    "            'reducer__n_components':reducer_var\n",
    "        }\n",
    "        grid = GridSearchCV(pipe,param_grid=param_grid,refit=True, verbose=0, return_train_score=True, cv=CV_SPLIT)\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(grid.best_estimator_.get_params())\n",
    "\n",
    "        return grid\n",
    "    else:\n",
    "        param_grid = {\n",
    "            \"C\": [100, 10, 1.0, 0.1, 0.01],\n",
    "            \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n",
    "            \"penalty\": [\"l1\", \"l2\"],\n",
    "        }\n",
    "        grid = GridSearchCV(\n",
    "            LogisticRegression(),\n",
    "            param_grid,\n",
    "            refit=True,\n",
    "            verbose=0,\n",
    "            return_train_score=True,\n",
    "            cv=CV_SPLIT,\n",
    "        )\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(grid.best_estimator_.get_params())\n",
    "        return grid\n",
    "\n",
    "\n",
    "def get_best_param_SVC(x_train, y_train,reducer_name):\n",
    "\n",
    "    if reducer_name != 'mrmr':\n",
    "        reducer = get_reducer_variables(reducer_name=reducer_name)\n",
    "        reducer_var = reducer['variables']\n",
    "        reducer_name = reducer['reducer']\n",
    "        \n",
    "        pipe = Pipeline(steps=[('reducer',reducer_name),('svc',SVC())])\n",
    "\n",
    "        param_grid = {\n",
    "            \"svc__C\": [0.1, 1, 10, 100],\n",
    "            \"svc__gamma\": [1, 0.1, 0.01, 0.001],\n",
    "            \"svc__kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "            'reducer__n_components':reducer_var\n",
    "        }\n",
    "        grid = GridSearchCV(pipe,param_grid=param_grid,refit=True, verbose=0, return_train_score=True, cv=CV_SPLIT)\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(grid.best_estimator_.get_params())\n",
    "\n",
    "        return grid\n",
    "    else:\n",
    "        param_grid = {\n",
    "            \"C\": [0.1, 1, 10, 100],\n",
    "            \"gamma\": [1, 0.1, 0.01, 0.001],\n",
    "            \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"]        \n",
    "        }\n",
    "        grid = GridSearchCV(\n",
    "            SVC(), param_grid, refit=True, verbose=0, return_train_score=True, cv=CV_SPLIT\n",
    "        )\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(grid.best_estimator_.get_params())\n",
    "        return grid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def division_function(n, d):\n",
    "    if d:\n",
    "        return n / d\n",
    "    elif n == 0 and d == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_model(model, X, Y):\n",
    "    \"\"\"\n",
    "    validates the model with a k-fold validation which is iterated\n",
    "    returns the mean accuracy, specificiy, recall, precision, f1 score and auc score\n",
    "    \"\"\"\n",
    "\n",
    "    splits = 5\n",
    "    iteration = 10\n",
    "\n",
    "    acc_list = []\n",
    "    specificity_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=splits)\n",
    "\n",
    "    # Iterate \"interation\" times of k-fold\n",
    "    for i in range(1, iteration):\n",
    "        # print(f'Iteration {i}/{iteration}')\n",
    "\n",
    "        acc_total = 0\n",
    "        specificity_total = 0\n",
    "        recall_total = 0\n",
    "        precision_total = 0\n",
    "        f1_total = 0\n",
    "\n",
    "        for train_index, test_index in folds.split(X, Y):\n",
    "            x_train = X.iloc[train_index, :]\n",
    "            x_test = X.iloc[test_index, :]\n",
    "            y_train = Y.iloc[train_index, :]\n",
    "            y_test = Y.iloc[test_index, :]\n",
    "\n",
    "            # scale\n",
    "            sc = MinMaxScaler()\n",
    "            x_train = sc.fit_transform(x_train)\n",
    "            x_test = sc.transform(x_test)\n",
    "\n",
    "            # fit model and predict\n",
    "            model.fit(x_train, np.ravel(y_train))\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            TN = conf_matrix[0][0]\n",
    "            FP = conf_matrix[0][1]\n",
    "            FN = conf_matrix[1][0]\n",
    "            TP = conf_matrix[1][1]\n",
    "\n",
    "            accuracy = (division_function((TP + TN), (TP + TN + FP + FN))) * 100\n",
    "            recall = division_function(TP, (TP + FN)) * 100  # recall\n",
    "            specificity = division_function(TN, (TN + FP)) * 100\n",
    "            precision = division_function(TP, (TP + FP)) * 100\n",
    "            f1_score = division_function(2 * (recall * precision), (recall + precision))\n",
    "\n",
    "            # sum it up\n",
    "            acc_total += accuracy\n",
    "            recall_total += recall\n",
    "            specificity_total += specificity\n",
    "            precision_total += precision\n",
    "            f1_total += f1_score\n",
    "\n",
    "        # avg\n",
    "        accuracy_mean = acc_total / splits\n",
    "        recall_mean = recall_total / splits\n",
    "        specificity_mean = specificity_total / splits\n",
    "        precision_mean = precision_total / splits\n",
    "        f1_mean = f1_total / splits\n",
    "\n",
    "        acc_list.append(accuracy_mean)\n",
    "        recall_list.append(recall_mean)\n",
    "        specificity_list.append(specificity_mean)\n",
    "        precision_list.append(precision_mean)\n",
    "        f1_list.append(f1_mean)\n",
    "\n",
    "    return (\n",
    "        np.mean(acc_list),\n",
    "        np.mean(specificity_list),\n",
    "        np.mean(recall_list),\n",
    "        np.mean(precision_list),\n",
    "        np.mean(f1_list),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_train, x_test, y_train, y_test,x_test_index, df):\n",
    "    # sc = MinMaxScaler()\n",
    "    # x_train = sc.fit_transform(x_train)\n",
    "    # x_test = sc.transform(x_test)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    ids = df.iloc[x_test_index]['id']\n",
    "    data = {'id':ids,'y_pred':y_pred,'y_test':y_test}\n",
    "    data_df = pd.DataFrame(data)\n",
    "    data_df_group = data_df.groupby(by=['id'],sort=False).mean().round()\n",
    "    y_pred_2 = data_df_group['y_pred']\n",
    "    y_test_2 = data_df_group['y_test']\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test_2, y_pred_2)\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    TP = conf_matrix[1][1]\n",
    "\n",
    "    accuracy = (division_function((TP + TN), (TP + TN + FP + FN))) * 100\n",
    "    recall = division_function(TP, (TP + FN)) * 100  # recall\n",
    "    specificity = division_function(TN, (TN + FP)) * 100\n",
    "    precision = division_function(TP, (TP + FP)) * 100\n",
    "    f1_score = division_function(2 * (recall * precision), (recall + precision))\n",
    "\n",
    "    return accuracy, recall, specificity, precision, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "\n",
    "path = '/Users/athena.kam/Documents/Thesis/codebase/thesis-2023-athena'\n",
    "os.chdir(path)\n",
    "\n",
    "\"\"\"\n",
    "Reducing the features, splitting the data into test and train, oversample the training data, train the model and validate and evaluate it\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read_and_split(\n",
    "    filename: str, isTranscript: bool, reduce: str, random_state: int, chunked: bool\n",
    "):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "\n",
    "    # Remove Personal Questions\n",
    "    df = df[df[\"personalQ\"] != 1].reset_index(drop=True)\n",
    "\n",
    "    headers = df.columns\n",
    "    non_embeddings_headers = []\n",
    "    for header in headers:\n",
    "        if header.find(\"embbedings\") < 0:\n",
    "            non_embeddings_headers.append(header)\n",
    "\n",
    "    X = df.drop(columns=non_embeddings_headers)\n",
    "    Y = df[\"classification\"]\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    x_index = range(len(X))\n",
    "    x_train_index, x_test_index, y_train, y_test = train_test_split(\n",
    "        x_index, Y, test_size=0.30, random_state=random_state\n",
    "    )\n",
    "    x_train = pd.DataFrame(X).iloc[x_train_index]\n",
    "    x_test = pd.DataFrame(X).iloc[x_test_index]\n",
    "\n",
    "    # sc = MinMaxScaler()\n",
    "    # x_train = sc.fit_transform(x_train)\n",
    "    # x_test = sc.transform(x_test)\n",
    "    # Oversample minority group\n",
    "    sm = SMOTE(random_state=12)\n",
    "    x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "    if reduce == \"mrmr\":\n",
    "        x_train,x_test = feature_reduction_mrmr(pd.DataFrame(x_train), pd.DataFrame(x_test), pd.DataFrame(y_train), 20)\n",
    "\n",
    "    #     # Feature Reduction\n",
    "    #     if reduce == \"pca\":\n",
    "    #         x_train,x_test = feature_reduction_pca(x_train, x_test, 0.9)\n",
    "    #     elif reduce == \"lda\":\n",
    "    #         x_train,x_test = feature_reduction_lda(x_train, x_test, y_train)\n",
    "    #     elif reduce == \"mrmr\":\n",
    "    #         x_train,x_test = feature_reduction_mrmr(pd.DataFrame(x_train), pd.DataFrame(x_test), pd.DataFrame(y_train), 30)\n",
    "\n",
    "    # else:\n",
    "    #     if chunked:\n",
    "    #         df.drop([\"voiceID\", \"label_x\"], inplace=True, axis=1)\n",
    "    #         df.rename(columns={\"label_y\": \"label\"}, inplace=True)\n",
    "    #     else:\n",
    "    #         df.drop([\"voiceID\"], inplace=True, axis=1)\n",
    "    #     df[\"label\"].value_counts()\n",
    "    #     df = df.dropna()\n",
    "\n",
    "    #     df_X = df.iloc[:, :-1]\n",
    "    #     df_Y = df.iloc[:, -1]\n",
    "\n",
    "    #     x_train, x_test, y_train, y_test = train_test_split(\n",
    "    #         df_X, df_Y, test_size=0.3, random_state=random_state\n",
    "    #     )\n",
    "\n",
    "    #     sc = MinMaxScaler()\n",
    "    #     x_train = sc.fit_transform(x_train)\n",
    "    #     x_test = sc.transform(x_test)\n",
    "    #     pd.DataFrame(x_train)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test,x_test_index,df\n",
    "\n",
    "\n",
    "def train_model(model_name: str, grid_search: bool, model_weights, x_train, y_train,reducer):\n",
    "    if model_name == \"svc\":\n",
    "        if grid_search:\n",
    "            grid = get_best_param_SVC(x_train=x_train, y_train=y_train,reducer_name=reducer)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            model = SVC(\n",
    "                C=model_weights[\"C\"],\n",
    "                gamma=model_weights[\"gamma\"],\n",
    "                kernel=model_weights[\"kernel\"],\n",
    "            )\n",
    "    elif model_name == \"lr\":\n",
    "        if grid_search:\n",
    "            grid = get_best_param_LR(x_train=x_train, y_train=y_train, reducer_name=reducer)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            model = LogisticRegression(\n",
    "                C=model_weights[\"C\"],\n",
    "                solver=model_weights[\"solver\"],\n",
    "                penalty=model_weights[\"penalty\"],\n",
    "            )\n",
    "    elif model_name == \"knn\":\n",
    "        if grid_search:\n",
    "            grid = get_best_param_KNN(x_train=x_train, y_train=y_train,reducer_name=reducer)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            model = KNeighborsClassifier(\n",
    "                n_neighbors=model_weights[\"n_neighbors\"],\n",
    "                weights=model_weights[\"weights\"],\n",
    "                metric=model_weights[\"metric\"],\n",
    "            )\n",
    "    elif model_name == \"rf\":\n",
    "        if grid_search:\n",
    "            grid = get_best_param_RF(x_train=x_train, y_train=y_train,reducer_name=reducer)\n",
    "            model = grid.best_estimator_\n",
    "        else:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=model_weights[\"n_estimators\"],\n",
    "                max_features=model_weights[\"max_features\"],\n",
    "                max_depth=model_weights[\"max_depth\"],\n",
    "                criterion=model_weights[\"criterion\"],\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_test_model(\n",
    "    filename: str,\n",
    "    model_name: str,\n",
    "    chunked: bool = False,\n",
    "    reduce: str = \"mrmr\",\n",
    "    isTranscript: bool = True,\n",
    "    grid_search: bool = True,\n",
    "    model_weights: dict = {},\n",
    "    random_state: int = 0,\n",
    "):\n",
    "    x_train, x_test, y_train, y_test,x_test_index,df = read_and_split(\n",
    "        filename=filename,\n",
    "        isTranscript=isTranscript,\n",
    "        reduce=reduce,\n",
    "        random_state=random_state,\n",
    "        chunked=chunked,\n",
    "    )\n",
    "\n",
    "    # Train ML model\n",
    "    model = train_model(\n",
    "        model_name=model_name,\n",
    "        grid_search=grid_search,\n",
    "        model_weights=model_weights,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        reducer=reduce    )\n",
    "\n",
    "    # Validate with training data\n",
    "    accuracy, specificiy, recall, precision, f1_score = validate_model(\n",
    "        model, pd.DataFrame(x_train), pd.DataFrame(y_train)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"\\tAverage Accuracy: {accuracy} \\n\\\n",
    "      Average Specificity: {specificiy} \\n\\\n",
    "      Average Recall: {recall}\\n\\\n",
    "      Average Precision:{precision}\\n\\\n",
    "      Average F1 score {f1_score}\\n\\\n",
    "      \"\n",
    "    )\n",
    "\n",
    "    # Test with test data\n",
    "    accuracy, specificiy, recall, precision, f1_score = evaluate_model(\n",
    "        model=model, x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test,x_test_index = x_test_index,df=df\n",
    "    )\n",
    "    print(\"___________________\")\n",
    "    print(\"Evaluate model\")\n",
    "    print(\n",
    "        f\"\\tAccuracy: {accuracy} \\n\\\n",
    "    Specificity: {specificiy} \\n\\\n",
    "    Recall: {recall}\\n\\\n",
    "    Precision:{precision}\\n\\\n",
    "    F1 score {f1_score}\\n\\\n",
    "    \"\n",
    "    )\n",
    "\n",
    "    return accuracy, specificiy, recall, precision, f1_score,model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('reducer', LinearDiscriminantAnalysis(n_components=1)), ('svc', SVC(C=100, gamma=1))], 'verbose': False, 'reducer': LinearDiscriminantAnalysis(n_components=1), 'svc': SVC(C=100, gamma=1), 'reducer__covariance_estimator': None, 'reducer__n_components': 1, 'reducer__priors': None, 'reducer__shrinkage': None, 'reducer__solver': 'svd', 'reducer__store_covariance': False, 'reducer__tol': 0.0001, 'svc__C': 100, 'svc__break_ties': False, 'svc__cache_size': 200, 'svc__class_weight': None, 'svc__coef0': 0.0, 'svc__decision_function_shape': 'ovr', 'svc__degree': 3, 'svc__gamma': 1, 'svc__kernel': 'rbf', 'svc__max_iter': -1, 'svc__probability': False, 'svc__random_state': None, 'svc__shrinking': True, 'svc__tol': 0.001, 'svc__verbose': False}\n",
      "\tAverage Accuracy: 71.33333333333333 \n",
      "      Average Specificity: 66.66666666666666 \n",
      "      Average Recall: 80.0\n",
      "      Average Precision:71.66666666666666\n",
      "      Average F1 score 73.14285714285714\n",
      "      \n",
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 54.54545454545454 \n",
      "    Specificity: 0.0 \n",
      "    Recall: 75.0\n",
      "    Precision:0.0\n",
      "    F1 score 0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "accuracy, specificiy, recall, precision, f1_score,model=train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv', model_name= 'svc',reduce='lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('reducer', PCA(n_components=0.8)), ('svc', SVC(C=1, gamma=0.1))], 'verbose': False, 'reducer': PCA(n_components=0.8), 'svc': SVC(C=1, gamma=0.1), 'reducer__copy': True, 'reducer__iterated_power': 'auto', 'reducer__n_components': 0.8, 'reducer__n_oversamples': 10, 'reducer__power_iteration_normalizer': 'auto', 'reducer__random_state': None, 'reducer__svd_solver': 'auto', 'reducer__tol': 0.0, 'reducer__whiten': False, 'svc__C': 1, 'svc__break_ties': False, 'svc__cache_size': 200, 'svc__class_weight': None, 'svc__coef0': 0.0, 'svc__decision_function_shape': 'ovr', 'svc__degree': 3, 'svc__gamma': 0.1, 'svc__kernel': 'rbf', 'svc__max_iter': -1, 'svc__probability': False, 'svc__random_state': None, 'svc__shrinking': True, 'svc__tol': 0.001, 'svc__verbose': False}\n",
      "\tAverage Accuracy: 83.80952380952381 \n",
      "      Average Specificity: 73.33333333333333 \n",
      "      Average Recall: 93.33333333333333\n",
      "      Average Precision:79.33333333333333\n",
      "      Average F1 score 85.39682539682539\n",
      "      \n",
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 63.63636363636363 \n",
      "    Specificity: 40.0 \n",
      "    Recall: 83.33333333333334\n",
      "    Precision:66.66666666666666\n",
      "    F1 score 49.99999999999999\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(63.63636363636363,\n",
       " 40.0,\n",
       " 83.33333333333334,\n",
       " 66.66666666666666,\n",
       " 49.99999999999999,\n",
       " Pipeline(steps=[('reducer', PCA(n_components=0.8)),\n",
       "                 ('svc', SVC(C=1, gamma=0.1))]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv', model_name= 'svc',reduce='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('reducer', PCA(n_components=0.8)), ('lr', LogisticRegression(C=100, solver='newton-cg'))], 'verbose': False, 'reducer': PCA(n_components=0.8), 'lr': LogisticRegression(C=100, solver='newton-cg'), 'reducer__copy': True, 'reducer__iterated_power': 'auto', 'reducer__n_components': 0.8, 'reducer__n_oversamples': 10, 'reducer__power_iteration_normalizer': 'auto', 'reducer__random_state': None, 'reducer__svd_solver': 'auto', 'reducer__tol': 0.0, 'reducer__whiten': False, 'lr__C': 100, 'lr__class_weight': None, 'lr__dual': False, 'lr__fit_intercept': True, 'lr__intercept_scaling': 1, 'lr__l1_ratio': None, 'lr__max_iter': 100, 'lr__multi_class': 'auto', 'lr__n_jobs': None, 'lr__penalty': 'l2', 'lr__random_state': None, 'lr__solver': 'newton-cg', 'lr__tol': 0.0001, 'lr__verbose': 0, 'lr__warm_start': False}\n",
      "\tAverage Accuracy: 78.0952380952381 \n",
      "      Average Specificity: 68.33333333333333 \n",
      "      Average Recall: 88.33333333333333\n",
      "      Average Precision:73.33333333333333\n",
      "      Average F1 score 79.76190476190476\n",
      "      \n",
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 81.81818181818183 \n",
      "    Specificity: 80.0 \n",
      "    Recall: 83.33333333333334\n",
      "    Precision:80.0\n",
      "    F1 score 80.0\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81.81818181818183,\n",
       " 80.0,\n",
       " 83.33333333333334,\n",
       " 80.0,\n",
       " 80.0,\n",
       " Pipeline(steps=[('reducer', PCA(n_components=0.8)),\n",
       "                 ('lr', LogisticRegression(C=100, solver='newton-cg'))]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv', model_name= 'lr',reduce='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('reduce', PCA(n_components=0.8)),\n",
    "                 ('svc', SVC(C=1, gamma=0.1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(n_components=0.8)\n"
     ]
    }
   ],
   "source": [
    "print(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter grid for parameter 'reduce__n_components' needs to be a list or a numpy array, but got 1 (of type int) instead. Single values need to be wrapped in a list with one element.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_test_model(filename\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdatasets/transformed/google/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, model_name\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39msvc\u001b[39;49m\u001b[39m'\u001b[39;49m,reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 155\u001b[0m, in \u001b[0;36mtrain_test_model\u001b[0;34m(filename, model_name, chunked, reduce, isTranscript, grid_search, model_weights, random_state)\u001b[0m\n\u001b[1;32m    146\u001b[0m x_train, x_test, y_train, y_test,x_test_index,df \u001b[39m=\u001b[39m read_and_split(\n\u001b[1;32m    147\u001b[0m     filename\u001b[39m=\u001b[39mfilename,\n\u001b[1;32m    148\u001b[0m     isTranscript\u001b[39m=\u001b[39misTranscript,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[39m# Train ML model\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m model \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m    156\u001b[0m     model_name\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[1;32m    157\u001b[0m     grid_search\u001b[39m=\u001b[39;49mgrid_search,\n\u001b[1;32m    158\u001b[0m     model_weights\u001b[39m=\u001b[39;49mmodel_weights,\n\u001b[1;32m    159\u001b[0m     x_train\u001b[39m=\u001b[39;49mx_train,\n\u001b[1;32m    160\u001b[0m     y_train\u001b[39m=\u001b[39;49my_train,\n\u001b[1;32m    161\u001b[0m     reducer\u001b[39m=\u001b[39;49mreduce\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[39m# Validate with training data\u001b[39;00m\n\u001b[1;32m    165\u001b[0m accuracy, specificiy, recall, precision, f1_score \u001b[39m=\u001b[39m validate_model(\n\u001b[1;32m    166\u001b[0m     model, pd\u001b[39m.\u001b[39mDataFrame(x_train), pd\u001b[39m.\u001b[39mDataFrame(y_train)\n\u001b[1;32m    167\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 93\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, grid_search, model_weights, x_train, y_train, reducer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msvc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m grid_search:\n\u001b[0;32m---> 93\u001b[0m         grid \u001b[39m=\u001b[39m get_best_param_SVC(x_train\u001b[39m=\u001b[39;49mx_train, y_train\u001b[39m=\u001b[39;49my_train,reducer_name\u001b[39m=\u001b[39;49mreducer)\n\u001b[1;32m     94\u001b[0m         model \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mbest_estimator_\n\u001b[1;32m     95\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[36], line 101\u001b[0m, in \u001b[0;36mget_best_param_SVC\u001b[0;34m(x_train, y_train, reducer_name)\u001b[0m\n\u001b[1;32m     94\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[1;32m     95\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msvc__C\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m100\u001b[39m],\n\u001b[1;32m     96\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msvc__gamma\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.001\u001b[39m],\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msvc__kernel\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpoly\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     98\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mreduce__n_components\u001b[39m\u001b[39m'\u001b[39m:reducer_var\n\u001b[1;32m     99\u001b[0m }\n\u001b[1;32m    100\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(pipe,param_grid\u001b[39m=\u001b[39mparam_grid,refit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cv\u001b[39m=\u001b[39mCV_SPLIT)\n\u001b[0;32m--> 101\u001b[0m grid\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[1;32m    102\u001b[0m \u001b[39mprint\u001b[39m(grid\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mget_params())\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m grid\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:117\u001b[0m, in \u001b[0;36mParameterGrid.__init__\u001b[0;34m(self, param_grid)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter array for \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m should be one-dimensional, got:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m with shape \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    115\u001b[0m     value, (np\u001b[39m.\u001b[39mndarray, Sequence)\n\u001b[1;32m    116\u001b[0m ):\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter grid for parameter \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m needs to be a list or a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m numpy array, but got \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m (of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) instead. Single values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mneed to be wrapped in a list with one element.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(value) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    125\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter grid for parameter \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m need \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto be a non-empty sequence, got: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Parameter grid for parameter 'reduce__n_components' needs to be a list or a numpy array, but got 1 (of type int) instead. Single values need to be wrapped in a list with one element."
     ]
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv', model_name= 'svc',reduce='lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'sigmoid', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\tAverage Accuracy: 97.49007936507937 \n",
      "      Average Specificity: 95.0 \n",
      "      Average Recall: 100.0\n",
      "      Average Precision:95.29106187929717\n",
      "      Average F1 score 97.56572469818738\n",
      "      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>ID35_hc_0_0_0_noPersonalQuestions.flac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>ID11_hc_0_0_0_noPersonalQ.flac</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>ID28_hc_0_0_0_noPersonalQ.flac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>ID22_hc_0_0_0_noPersonalQ.flac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>ID15_hc_0_0_0_noPersonalQ.flac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>ID10_hc_0_0_0_noPersonalQ.flac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ID00_hc_0_0_0.flac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ID07_pd_2_0_0_noPersonalQ.flac</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>ID25_hc_0_0_0_noPersonalQ.flac</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>ID35_hc_0_0_0_noPersonalQuestions.flac</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  y_pred  y_test\n",
       "366  ID35_hc_0_0_0_noPersonalQuestions.flac       0       0\n",
       "155          ID11_hc_0_0_0_noPersonalQ.flac       1       0\n",
       "313          ID28_hc_0_0_0_noPersonalQ.flac       0       0\n",
       "269          ID22_hc_0_0_0_noPersonalQ.flac       0       0\n",
       "221          ID15_hc_0_0_0_noPersonalQ.flac       0       0\n",
       "..                                      ...     ...     ...\n",
       "144          ID10_hc_0_0_0_noPersonalQ.flac       0       0\n",
       "12                       ID00_hc_0_0_0.flac       0       0\n",
       "101          ID07_pd_2_0_0_noPersonalQ.flac       0       1\n",
       "293          ID25_hc_0_0_0_noPersonalQ.flac       0       0\n",
       "365  ID35_hc_0_0_0_noPersonalQuestions.flac       1       0\n",
       "\n",
       "[114 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "id\n",
       "ID35_hc_0_0_0_noPersonalQuestions.flac    1.0\n",
       "ID11_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID28_hc_0_0_0_noPersonalQ.flac            1.0\n",
       "ID22_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID15_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID05_hc_0_0_0.flac                        0.0\n",
       "ID33_pd_3_2_2.flac                        0.0\n",
       "ID04_pd_2_0_1_noPersonalQ.flac            0.0\n",
       "ID09_hc_0_0_0.flac                        0.0\n",
       "ID03_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID23_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID20_pd_3_0_1_noPersonalQ.flac            1.0\n",
       "ID10_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID16_pd_2_0_0_noPersonalQ.flac            1.0\n",
       "ID27_pd_4_1_1_noPersonalQ.flac            0.0\n",
       "ID26_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID29_pd_3_1_2_noPersonalQ.flac            1.0\n",
       "ID07_pd_2_0_0_noPersonalQ.flac            0.0\n",
       "ID13_pd_3_2_2.flac                        0.0\n",
       "ID17_pd_2_1_0.flac                        1.0\n",
       "ID06_pd_3_1_1.flac                        0.0\n",
       "ID36_hc_0_0_0_noPersonalQ.flac            1.0\n",
       "ID00_hc_0_0_0.flac                        0.0\n",
       "ID24_pd_2_0_0_noPersonalQ.flac            1.0\n",
       "ID02_pd_2_0_0.flac                        0.0\n",
       "ID12_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID01_hc_0_0_0.flac                        0.0\n",
       "ID05_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID08_hc_0_0_0.flac                        0.0\n",
       "ID14_hc_0_0_0.flac                        0.0\n",
       "ID25_hc_0_0_0_noPersonalQ.flac            0.0\n",
       "ID31_hc_0_1_1.flac                        0.0\n",
       "Name: y_pred, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 60.526315789473685 \n",
      "    Specificity: 50.0 \n",
      "    Recall: 66.66666666666666\n",
      "    Precision:46.666666666666664\n",
      "    F1 score 48.275862068965516\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60.526315789473685,\n",
       " 50.0,\n",
       " 66.66666666666666,\n",
       " 46.666666666666664,\n",
       " 48.275862068965516)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_sentence_embeddings_transformed.csv', model_name= 'svc',reduce='lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'sigmoid', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\tAverage Accuracy: 97.49007936507937 \n",
      "      Average Specificity: 95.0 \n",
      "      Average Recall: 100.0\n",
      "      Average Precision:95.29106187929717\n",
      "      Average F1 score 97.56572469818738\n",
      "      \n",
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 68.75 \n",
      "    Specificity: 41.66666666666667 \n",
      "    Recall: 85.0\n",
      "    Precision:62.5\n",
      "    F1 score 50.0\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(68.75, 41.66666666666667, 85.0, 62.5, 50.0)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_sentence_embeddings_transformed.csv', model_name= 'svc',reduce='lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "smth = {'reducer':PCA(),'variables':[0.8,0.9,0.95]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.9, 0.95]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smth['variables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:05<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'manhattan', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}\n",
      "\tAverage Accuracy: 96.66666666666667 \n",
      "      Average Specificity: 93.33333333333333 \n",
      "      Average Recall: 100.0\n",
      "      Average Precision:95.0\n",
      "      Average F1 score 97.14285714285714\n",
      "      \n",
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 63.63636363636363 \n",
      "    Specificity: 80.0 \n",
      "    Recall: 50.0\n",
      "    Precision:57.14285714285714\n",
      "    F1 score 66.66666666666666\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(63.63636363636363,\n",
       " 80.0,\n",
       " 50.0,\n",
       " 57.14285714285714,\n",
       " 66.66666666666666,\n",
       " KNeighborsClassifier(metric='manhattan', n_neighbors=3))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv', model_name= 'knn',reduce='mrmr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('reducer', LinearDiscriminantAnalysis(n_components=1)), ('rf', RandomForestClassifier(criterion='entropy', max_depth=4, max_features='log2'))], 'verbose': False, 'reducer': LinearDiscriminantAnalysis(n_components=1), 'rf': RandomForestClassifier(criterion='entropy', max_depth=4, max_features='log2'), 'reducer__covariance_estimator': None, 'reducer__n_components': 1, 'reducer__priors': None, 'reducer__shrinkage': None, 'reducer__solver': 'svd', 'reducer__store_covariance': False, 'reducer__tol': 0.0001, 'rf__bootstrap': True, 'rf__ccp_alpha': 0.0, 'rf__class_weight': None, 'rf__criterion': 'entropy', 'rf__max_depth': 4, 'rf__max_features': 'log2', 'rf__max_leaf_nodes': None, 'rf__max_samples': None, 'rf__min_impurity_decrease': 0.0, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__min_weight_fraction_leaf': 0.0, 'rf__n_estimators': 100, 'rf__n_jobs': None, 'rf__oob_score': False, 'rf__random_state': None, 'rf__verbose': 0, 'rf__warm_start': False}\n",
      "\tAverage Accuracy: 63.22751322751323 \n",
      "      Average Specificity: 73.33333333333333 \n",
      "      Average Recall: 53.888888888888886\n",
      "      Average Precision:69.07407407407408\n",
      "      Average F1 score 56.698412698412696\n",
      "      \n",
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 81.81818181818183 \n",
      "    Specificity: 100.0 \n",
      "    Recall: 66.66666666666666\n",
      "    Precision:71.42857142857143\n",
      "    F1 score 83.33333333333333\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81.81818181818183,\n",
       " 100.0,\n",
       " 66.66666666666666,\n",
       " 71.42857142857143,\n",
       " 83.33333333333333,\n",
       " Pipeline(steps=[('reducer', LinearDiscriminantAnalysis(n_components=1)),\n",
       "                 ('rf',\n",
       "                  RandomForestClassifier(criterion='entropy', max_depth=4,\n",
       "                                         max_features='log2'))]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/google/spontaneousDialogueOnly_google_bert_embeddings_transformed.csv', model_name= 'rf',reduce='lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'poly', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\tAverage Accuracy: 96.66666666666667 \n",
      "      Average Specificity: 93.33333333333333 \n",
      "      Average Recall: 100.0\n",
      "      Average Precision:95.0\n",
      "      Average F1 score 97.14285714285714\n",
      "      \n",
      "___________________\n",
      "Evaluate model\n",
      "\tAccuracy: 63.63636363636363 \n",
      "    Specificity: 25.0 \n",
      "    Recall: 85.71428571428571\n",
      "    Precision:50.0\n",
      "    F1 score 33.333333333333336\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(63.63636363636363,\n",
       " 25.0,\n",
       " 85.71428571428571,\n",
       " 50.0,\n",
       " 33.333333333333336,\n",
       " SVC(C=0.1, gamma=1, kernel='poly'))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(filename= 'datasets/transformed/whisper/spontaneousDialogueOnly_whisper_bert_embeddings_transformed.csv', model_name= 'svc',reduce='mrmr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
