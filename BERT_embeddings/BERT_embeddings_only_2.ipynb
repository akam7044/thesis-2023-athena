{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "412914.26s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/athena.kam/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from time import ctime\n",
    "import tracemalloc\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create embeddings \n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tokens \n",
    "def create_tokens(filename:str):\n",
    "    print(\"creating tokens\")\n",
    "    print(ctime())\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    csv_file = filename+'.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    transcripts = df.transcripts.values\n",
    "\n",
    "    for transcript in transcripts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                        transcript,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 430,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    # Convert the lists into tensors.\n",
    "    df[\"input_ids\"] = input_ids\n",
    "    df[\"attention_masks\"] = attention_masks   \n",
    "\n",
    "    return df\n",
    "\n",
    "def create_embeddings(input_id,mask):\n",
    "    outputs = model(input_id, mask)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # `token_vecs` is a tensor with shape [430 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the sentence token\n",
    "    return torch.mean(token_vecs, dim=0) \n",
    "\n",
    "def collect_embeddings(filename:str,save_csv:bool):\n",
    "\n",
    "    df = create_tokens(filename)\n",
    "\n",
    "    print(\"creating embeddings\")\n",
    "    print(ctime())\n",
    "    embeddings = []\n",
    "    rows = len(df)\n",
    "    tracemalloc.start()\n",
    "    for row in range(rows):\n",
    "        print(f\"{row} of {rows}\")\n",
    "        input_id = df.iloc[row].input_ids.reshape(1,430)\n",
    "        mask = df.iloc[row].attention_masks.reshape(1,430)\n",
    "        embeddings.append(create_embeddings(input_id,mask).detach().numpy())\n",
    "        print(\"done\")\n",
    "\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    df[\"embeddings\"] = embeddings\n",
    "    if save_csv:\n",
    "        out_filename = filename+'_bert_embeddings.csv'\n",
    "        df.to_csv(out_filename)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/athena.kam/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating tokens\n",
      "Mon Jun 19 16:24:00 2023\n",
      "creating embeddings\n",
      "Mon Jun 19 16:24:00 2023\n",
      "0 of 57\n",
      "done\n",
      "1 of 57\n",
      "done\n",
      "2 of 57\n",
      "done\n",
      "3 of 57\n",
      "done\n",
      "4 of 57\n",
      "done\n",
      "5 of 57\n",
      "done\n",
      "6 of 57\n",
      "done\n",
      "7 of 57\n",
      "done\n",
      "8 of 57\n",
      "done\n",
      "9 of 57\n",
      "done\n",
      "10 of 57\n",
      "done\n",
      "11 of 57\n",
      "done\n",
      "12 of 57\n",
      "done\n",
      "13 of 57\n",
      "done\n",
      "14 of 57\n",
      "done\n",
      "15 of 57\n",
      "done\n",
      "16 of 57\n",
      "done\n",
      "17 of 57\n",
      "done\n",
      "18 of 57\n",
      "done\n",
      "19 of 57\n",
      "done\n",
      "20 of 57\n",
      "done\n",
      "21 of 57\n",
      "done\n",
      "22 of 57\n",
      "done\n",
      "23 of 57\n",
      "done\n",
      "24 of 57\n",
      "done\n",
      "25 of 57\n",
      "done\n",
      "26 of 57\n",
      "done\n",
      "27 of 57\n",
      "done\n",
      "28 of 57\n",
      "done\n",
      "29 of 57\n",
      "done\n",
      "30 of 57\n",
      "done\n",
      "31 of 57\n",
      "done\n",
      "32 of 57\n",
      "done\n",
      "33 of 57\n",
      "done\n",
      "34 of 57\n",
      "done\n",
      "35 of 57\n",
      "done\n",
      "36 of 57\n",
      "done\n",
      "37 of 57\n",
      "done\n",
      "38 of 57\n",
      "done\n",
      "39 of 57\n",
      "done\n",
      "40 of 57\n",
      "done\n",
      "41 of 57\n",
      "done\n",
      "42 of 57\n",
      "done\n",
      "43 of 57\n",
      "done\n",
      "44 of 57\n",
      "done\n",
      "45 of 57\n",
      "done\n",
      "46 of 57\n",
      "done\n",
      "47 of 57\n",
      "done\n",
      "48 of 57\n",
      "done\n",
      "49 of 57\n",
      "done\n",
      "50 of 57\n",
      "done\n",
      "51 of 57\n",
      "done\n",
      "52 of 57\n",
      "done\n",
      "53 of 57\n",
      "done\n",
      "54 of 57\n",
      "done\n",
      "55 of 57\n",
      "done\n",
      "56 of 57\n",
      "done\n",
      "Current memory usage is 0.038747MB; Peak was 0.048455MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcripts</th>\n",
       "      <th>classification</th>\n",
       "      <th>noPersonalQ</th>\n",
       "      <th>personalQ</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_masks</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00_hc_0_0_0.wav</td>\n",
       "      <td>Yeah, in London you can go to Oxford Street, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(3398), tensor(1010), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.07400678, -0.40879136, 0.40594617, -0.05566...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID01_hc_0_0_0.wav</td>\n",
       "      <td>Okay. Okay. Yes. Okay. So you want to know wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(3100), tensor(1012), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.13023636, -0.5625998, 0.4663836, -0.0846741...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID02_pd_2_0_0.wav</td>\n",
       "      <td>So this is your first time in London, you've ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(2061), tensor(2023), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.00093209464, -0.27498776, 0.28973624, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID03_hc_0_0_0_noPersonalQ.wav</td>\n",
       "      <td>Okay, so I'm much very long than you're here....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(3100), tensor(1010), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.09951415, -0.39558145, 0.8188276, -0.263874...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID03_hc_0_0_0.wav</td>\n",
       "      <td>Okay, so I'm much very long than you're here....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[tensor(101), tensor(3100), tensor(1010), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.053875744, -0.3047915, 0.7414313, -0.202025...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id   \n",
       "0              ID00_hc_0_0_0.wav  \\\n",
       "1              ID01_hc_0_0_0.wav   \n",
       "2              ID02_pd_2_0_0.wav   \n",
       "3  ID03_hc_0_0_0_noPersonalQ.wav   \n",
       "4              ID03_hc_0_0_0.wav   \n",
       "\n",
       "                                         transcripts  classification   \n",
       "0   Yeah, in London you can go to Oxford Street, ...               0  \\\n",
       "1   Okay. Okay. Yes. Okay. So you want to know wh...               0   \n",
       "2   So this is your first time in London, you've ...               1   \n",
       "3   Okay, so I'm much very long than you're here....               0   \n",
       "4   Okay, so I'm much very long than you're here....               0   \n",
       "\n",
       "   noPersonalQ  personalQ                                          input_ids   \n",
       "0            0          0  [[tensor(101), tensor(3398), tensor(1010), ten...  \\\n",
       "1            0          0  [[tensor(101), tensor(3100), tensor(1012), ten...   \n",
       "2            0          0  [[tensor(101), tensor(2061), tensor(2023), ten...   \n",
       "3            1          0  [[tensor(101), tensor(3100), tensor(1010), ten...   \n",
       "4            0          1  [[tensor(101), tensor(3100), tensor(1010), ten...   \n",
       "\n",
       "                                     attention_masks   \n",
       "0  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \\\n",
       "1  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "2  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "3  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "4  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.07400678, -0.40879136, 0.40594617, -0.05566...  \n",
       "1  [0.13023636, -0.5625998, 0.4663836, -0.0846741...  \n",
       "2  [-0.00093209464, -0.27498776, 0.28973624, -0.1...  \n",
       "3  [0.09951415, -0.39558145, 0.8188276, -0.263874...  \n",
       "4  [0.053875744, -0.3047915, 0.7414313, -0.202025...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embeddings_whisper = collect_embeddings(\"spontaneousDialogueOnly_whisper\",False)\n",
    "df_embeddings_whisper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating tokens\n",
      "Mon Jun 19 16:24:26 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/athena.kam/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating embeddings\n",
      "Mon Jun 19 16:24:27 2023\n",
      "0 of 57\n",
      "done\n",
      "1 of 57\n",
      "done\n",
      "2 of 57\n",
      "done\n",
      "3 of 57\n",
      "done\n",
      "4 of 57\n",
      "done\n",
      "5 of 57\n",
      "done\n",
      "6 of 57\n",
      "done\n",
      "7 of 57\n",
      "done\n",
      "8 of 57\n",
      "done\n",
      "9 of 57\n",
      "done\n",
      "10 of 57\n",
      "done\n",
      "11 of 57\n",
      "done\n",
      "12 of 57\n",
      "done\n",
      "13 of 57\n",
      "done\n",
      "14 of 57\n",
      "done\n",
      "15 of 57\n",
      "done\n",
      "16 of 57\n",
      "done\n",
      "17 of 57\n",
      "done\n",
      "18 of 57\n",
      "done\n",
      "19 of 57\n",
      "done\n",
      "20 of 57\n",
      "done\n",
      "21 of 57\n",
      "done\n",
      "22 of 57\n",
      "done\n",
      "23 of 57\n",
      "done\n",
      "24 of 57\n",
      "done\n",
      "25 of 57\n",
      "done\n",
      "26 of 57\n",
      "done\n",
      "27 of 57\n",
      "done\n",
      "28 of 57\n",
      "done\n",
      "29 of 57\n",
      "done\n",
      "30 of 57\n",
      "done\n",
      "31 of 57\n",
      "done\n",
      "32 of 57\n",
      "done\n",
      "33 of 57\n",
      "done\n",
      "34 of 57\n",
      "done\n",
      "35 of 57\n",
      "done\n",
      "36 of 57\n",
      "done\n",
      "37 of 57\n",
      "done\n",
      "38 of 57\n",
      "done\n",
      "39 of 57\n",
      "done\n",
      "40 of 57\n",
      "done\n",
      "41 of 57\n",
      "done\n",
      "42 of 57\n",
      "done\n",
      "43 of 57\n",
      "done\n",
      "44 of 57\n",
      "done\n",
      "45 of 57\n",
      "done\n",
      "46 of 57\n",
      "done\n",
      "47 of 57\n",
      "done\n",
      "48 of 57\n",
      "done\n",
      "49 of 57\n",
      "done\n",
      "50 of 57\n",
      "done\n",
      "51 of 57\n",
      "done\n",
      "52 of 57\n",
      "done\n",
      "53 of 57\n",
      "done\n",
      "54 of 57\n",
      "done\n",
      "55 of 57\n",
      "done\n",
      "56 of 57\n",
      "done\n",
      "Current memory usage is 0.034012MB; Peak was 0.043664MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcripts</th>\n",
       "      <th>classification</th>\n",
       "      <th>noPersonalQ</th>\n",
       "      <th>personalQ</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_masks</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00_hc_0_0_0.wav</td>\n",
       "      <td>Ye m in london ygoi to  oxford streets which i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(6300), tensor(1049), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.17662367, -0.008225093, 0.64076936, -0.262...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID01_hc_0_0_0.wav</td>\n",
       "      <td>O k k yes o k am am so you want you want to kn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(1051), tensor(1047), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.40270266, 0.045286078, 0.857207, -0.373066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID02_pd_2_0_0.wav</td>\n",
       "      <td>So this is your your first time in london you'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(2061), tensor(2023), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.3046138, 0.037615173, 0.39422023, -0.38926...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID03_hc_0_0_0_noPersonalQ.wav</td>\n",
       "      <td>E i so i am not very  londoner here i camp her...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(1041), tensor(1045), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.19965193, -0.20022212, 0.645765, -0.282061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID03_hc_0_0_0.wav</td>\n",
       "      <td>I iso i am not very londoner here i came here ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[tensor(101), tensor(1045), tensor(11163), te...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.24024455, -0.1889159, 0.81587195, -0.39491...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id   \n",
       "0              ID00_hc_0_0_0.wav  \\\n",
       "1              ID01_hc_0_0_0.wav   \n",
       "2              ID02_pd_2_0_0.wav   \n",
       "3  ID03_hc_0_0_0_noPersonalQ.wav   \n",
       "4              ID03_hc_0_0_0.wav   \n",
       "\n",
       "                                         transcripts  classification   \n",
       "0  Ye m in london ygoi to  oxford streets which i...               0  \\\n",
       "1  O k k yes o k am am so you want you want to kn...               0   \n",
       "2  So this is your your first time in london you'...               1   \n",
       "3  E i so i am not very  londoner here i camp her...               0   \n",
       "4  I iso i am not very londoner here i came here ...               0   \n",
       "\n",
       "   noPersonalQ  personalQ                                          input_ids   \n",
       "0            0          0  [[tensor(101), tensor(6300), tensor(1049), ten...  \\\n",
       "1            0          0  [[tensor(101), tensor(1051), tensor(1047), ten...   \n",
       "2            0          0  [[tensor(101), tensor(2061), tensor(2023), ten...   \n",
       "3            1          0  [[tensor(101), tensor(1041), tensor(1045), ten...   \n",
       "4            0          1  [[tensor(101), tensor(1045), tensor(11163), te...   \n",
       "\n",
       "                                     attention_masks   \n",
       "0  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \\\n",
       "1  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "2  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "3  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "4  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.17662367, -0.008225093, 0.64076936, -0.262...  \n",
       "1  [-0.40270266, 0.045286078, 0.857207, -0.373066...  \n",
       "2  [-0.3046138, 0.037615173, 0.39422023, -0.38926...  \n",
       "3  [-0.19965193, -0.20022212, 0.645765, -0.282061...  \n",
       "4  [-0.24024455, -0.1889159, 0.81587195, -0.39491...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embeddings_wav2vec = collect_embeddings(\"spontaneousDialogueOnly_wav2vec\",False)\n",
    "df_embeddings_wav2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating tokens\n",
      "Mon Jun 19 16:24:53 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/athena.kam/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating embeddings\n",
      "Mon Jun 19 16:24:53 2023\n",
      "0 of 57\n",
      "done\n",
      "1 of 57\n",
      "done\n",
      "2 of 57\n",
      "done\n",
      "3 of 57\n",
      "done\n",
      "4 of 57\n",
      "done\n",
      "5 of 57\n",
      "done\n",
      "6 of 57\n",
      "done\n",
      "7 of 57\n",
      "done\n",
      "8 of 57\n",
      "done\n",
      "9 of 57\n",
      "done\n",
      "10 of 57\n",
      "done\n",
      "11 of 57\n",
      "done\n",
      "12 of 57\n",
      "done\n",
      "13 of 57\n",
      "done\n",
      "14 of 57\n",
      "done\n",
      "15 of 57\n",
      "done\n",
      "16 of 57\n",
      "done\n",
      "17 of 57\n",
      "done\n",
      "18 of 57\n",
      "done\n",
      "19 of 57\n",
      "done\n",
      "20 of 57\n",
      "done\n",
      "21 of 57\n",
      "done\n",
      "22 of 57\n",
      "done\n",
      "23 of 57\n",
      "done\n",
      "24 of 57\n",
      "done\n",
      "25 of 57\n",
      "done\n",
      "26 of 57\n",
      "done\n",
      "27 of 57\n",
      "done\n",
      "28 of 57\n",
      "done\n",
      "29 of 57\n",
      "done\n",
      "30 of 57\n",
      "done\n",
      "31 of 57\n",
      "done\n",
      "32 of 57\n",
      "done\n",
      "33 of 57\n",
      "done\n",
      "34 of 57\n",
      "done\n",
      "35 of 57\n",
      "done\n",
      "36 of 57\n",
      "done\n",
      "37 of 57\n",
      "done\n",
      "38 of 57\n",
      "done\n",
      "39 of 57\n",
      "done\n",
      "40 of 57\n",
      "done\n",
      "41 of 57\n",
      "done\n",
      "42 of 57\n",
      "done\n",
      "43 of 57\n",
      "done\n",
      "44 of 57\n",
      "done\n",
      "45 of 57\n",
      "done\n",
      "46 of 57\n",
      "done\n",
      "47 of 57\n",
      "done\n",
      "48 of 57\n",
      "done\n",
      "49 of 57\n",
      "done\n",
      "50 of 57\n",
      "done\n",
      "51 of 57\n",
      "done\n",
      "52 of 57\n",
      "done\n",
      "53 of 57\n",
      "done\n",
      "54 of 57\n",
      "done\n",
      "55 of 57\n",
      "done\n",
      "56 of 57\n",
      "done\n",
      "Current memory usage is 0.033002MB; Peak was 0.04271MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcripts</th>\n",
       "      <th>classification</th>\n",
       "      <th>noPersonalQ</th>\n",
       "      <th>personalQ</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_masks</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00_hc_0_0_0.flac</td>\n",
       "      <td>Yeah, I'm in London. You can go to Oxford Stre...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(3398), tensor(1010), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.083030276, -0.21260752, 0.54635304, -0.1377...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID01_hc_0_0_0.flac</td>\n",
       "      <td>Okay.  Okay. Yes. Okay, so you want you want t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(3100), tensor(1012), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.07697948, -0.41965404, 0.55615896, 0.061804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID02_pd_2_0_0.flac</td>\n",
       "      <td>So this is your your first time in London, you...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(2061), tensor(2023), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[-0.037038386, -0.2024736, 0.45683667, -0.0652...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID03_hc_0_0_0_noPersonalQ.flac</td>\n",
       "      <td>Okay. So I'm a Londoner fewer. I came here for...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[tensor(101), tensor(3100), tensor(1012), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.1356038, -0.57575417, 0.6849178, -0.0581971...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID03_hc_0_0_0.flac</td>\n",
       "      <td>Okay. So I'm a Londoner fewer. I came here for...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[tensor(101), tensor(3100), tensor(1012), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "      <td>[0.032855537, -0.6075742, 0.65772057, -0.02078...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id   \n",
       "0              ID00_hc_0_0_0.flac  \\\n",
       "1              ID01_hc_0_0_0.flac   \n",
       "2              ID02_pd_2_0_0.flac   \n",
       "3  ID03_hc_0_0_0_noPersonalQ.flac   \n",
       "4              ID03_hc_0_0_0.flac   \n",
       "\n",
       "                                         transcripts  classification   \n",
       "0  Yeah, I'm in London. You can go to Oxford Stre...               0  \\\n",
       "1  Okay.  Okay. Yes. Okay, so you want you want t...               0   \n",
       "2  So this is your your first time in London, you...               1   \n",
       "3  Okay. So I'm a Londoner fewer. I came here for...               0   \n",
       "4  Okay. So I'm a Londoner fewer. I came here for...               0   \n",
       "\n",
       "   noPersonalQ  personalQ                                          input_ids   \n",
       "0            0          0  [[tensor(101), tensor(3398), tensor(1010), ten...  \\\n",
       "1            0          0  [[tensor(101), tensor(3100), tensor(1012), ten...   \n",
       "2            0          0  [[tensor(101), tensor(2061), tensor(2023), ten...   \n",
       "3            1          0  [[tensor(101), tensor(3100), tensor(1012), ten...   \n",
       "4            0          1  [[tensor(101), tensor(3100), tensor(1012), ten...   \n",
       "\n",
       "                                     attention_masks   \n",
       "0  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \\\n",
       "1  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "2  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "3  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "4  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.083030276, -0.21260752, 0.54635304, -0.1377...  \n",
       "1  [0.07697948, -0.41965404, 0.55615896, 0.061804...  \n",
       "2  [-0.037038386, -0.2024736, 0.45683667, -0.0652...  \n",
       "3  [0.1356038, -0.57575417, 0.6849178, -0.0581971...  \n",
       "4  [0.032855537, -0.6075742, 0.65772057, -0.02078...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embeddings_google = collect_embeddings(\"spontaneousDialogueOnly_google\",False)\n",
    "df_embeddings_google.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
